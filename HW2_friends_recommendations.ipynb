{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# CS6220 Homework 2\n",
        "# Map Reduce: Friends of Friends\n",
        "\n",
        "Include your code in this file. Make sure the below piece of code is at the top, as we will use that variable for testing.\n",
        "\n",
        "### Tips and tricks\n",
        "* Besides the Spark documentation, use the REPL feature heavily, since you'll be able to see functionality and functions.\n",
        "* One function you may find useful is the `collect()` function that can collects the RDD from all machines and brings it into memory. This is only feasible for small datasets, and it will allow you to effectively debug.\n",
        "* You can mount the `datapath` from a Google Drive. That way you won't have to keep uploading to Google Colab.\n",
        "  * Try using the following code block:\n",
        "  \n",
        "  ```\n",
        "     from google.colab import drive\n",
        "     drive.mount('/content/drive')\n",
        "  ```\n",
        "\n",
        "* The total runtime is around 10 minutes, where you'll only notice in the reduce step. Spark is a lazy evaluator, and only when there's a `collect` or other evaluator step will you notice the lag."
      ],
      "metadata": {
        "id": "Qf3gvnCbpU-V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETstejp7pMWG"
      },
      "outputs": [],
      "source": [
        "#@title Data path for file. We will use the variable `data_path` for grading.\n",
        "datapath=\"/content/drive/MyDrive/path-to-soc-LiveJournal1Adj.txt\" #@param "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qnlLB8ncF0uX",
        "outputId": "7b208b8b-b946-4dcf-850f-5375a1249acf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "sample_data = open(\"small_sample.txt\", \"w\")  # append mode\n",
        "sample_data.write(\"1\\t2,3,5\\n\")\n",
        "sample_data.write(\"2\\t1,3,5,6\\n\")\n",
        "sample_data.write(\"3\\t1,2,7\\n\")\n",
        "sample_data.write(\"4\\t5,6,7\\n\")\n",
        "sample_data.write(\"5\\t1,2,4\\n\")\n",
        "sample_data.write(\"6\\t2,4,7\\n\")\n",
        "sample_data.write(\"7\\t3,4,6\\n\")\n",
        "sample_data.close()"
      ],
      "metadata": {
        "id": "ul8_gtOa_qVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Your Code Below\n",
        "\n",
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "id": "9RwApIVGpbpQ",
        "outputId": "dd141421-d946-45db-e775-7ca685e81e56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=2b943f0f517c712b5bc827c35075fbab5caa882fa5d33e59499ab55e60f5a305\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-510\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 27 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 143 MB of additional disk space will be used.\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 129496 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u352-ga-1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u352-ga-1~20.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u352-ga-1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u352-ga-1~20.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u352-ga-1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u352-ga-1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "# from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext\n",
        "import pandas as pd\n",
        "import itertools \n",
        "\n",
        "# create the Spark Session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# create the Spark Context\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "n27_0aIn_Wda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read data file and process data"
      ],
      "metadata": {
        "id": "WqPOebJD_c73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines = sc.textFile(datapath, 1).map(lambda x: x.split(\"\\t\"))\n"
      ],
      "metadata": {
        "id": "m9sqoMLhRJUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turn data type into int(for sorting after)\n",
        "# split friends into a list\n",
        "def split_to_int(split):\n",
        "  \n",
        "  user = int(split[0])\n",
        "  if len(split[1]) == 0:\n",
        "    friends = []\n",
        "  else:\n",
        "    friends = list(map(lambda x: int(x), split[1].split(\",\")))\n",
        "\n",
        "  return (user, friends)\n",
        "\n",
        "\n",
        "def make_pair(friend_list):\n",
        "  user = friend_list[0]\n",
        "  friends = friend_list[1]\n",
        "\n",
        "  already_friends = [((user, friend), 0) for friend in friends]\n",
        "  have_mutual_friend = [(have_mutual, 1)for have_mutual in itertools.combinations(friends, 2)]\n",
        "\n",
        "  return already_friends+have_mutual_friend\n",
        "\n",
        "\n",
        "\n",
        "def rec_for_both(rec):\n",
        "  users = rec[0]\n",
        "  count = rec[1]\n",
        "\n",
        "  user_1 = users[0]\n",
        "  user_2 = users[1]\n",
        "\n",
        "  rec_1 = (user_1, (user_2, count))\n",
        "  rec_2 = (user_2, (user_1, count))\n",
        "\n",
        "  return [rec_1, rec_2]\n",
        "\n",
        "\n",
        "def rec_to_sorted(recs):\n",
        "  recs.sort(key=lambda x: (-x[1], x[0]))\n",
        "  return list(map(lambda x: x[0], recs))[:10]\n",
        "\n",
        "\n",
        "\n",
        "# [user:int, friends:list[int]]\n",
        "user_friend_pair = lines.map(split_to_int).flatMap(make_pair)\n",
        "user_friend_pair.cache()\n",
        "\n",
        "mutual_friend_counts = user_friend_pair.groupByKey() \\\n",
        "    .filter(lambda edge: 0 not in edge[1]) \\\n",
        "    .map(lambda edge: (edge[0], sum(edge[1])))\n",
        "    \n",
        "\n",
        "recommend_list = mutual_friend_counts.flatMap(rec_for_both)\\\n",
        "    .groupByKey()\\\n",
        "    .map(lambda m: (m[0], rec_to_sorted(list(m[1]))))\n",
        "\n",
        "\n",
        "sample_list = [11, 924, 8941, 8942, 9019, 9020, 9021, 9022, 9990, 9992, 9993]\n",
        "\n",
        "sample_output = recommend_list.filter(lambda x: x[0] in sample_list)\n",
        "\n",
        "sample_output.collect()\n",
        "\n"
      ],
      "metadata": {
        "id": "LOAJzWHaP9CG",
        "outputId": "01a29b60-26ad-4ece-ad47-b1ea68e969cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(9021, [9020, 9016, 9017, 9022, 317, 9023]),\n",
              " (11, [27552, 7785, 27573, 27574, 27589, 27590, 27600, 27617, 27620, 27667]),\n",
              " (9990, [13134, 13478, 13877, 34299, 34485, 34642, 37941]),\n",
              " (8942, [8939, 8940, 8943, 8944]),\n",
              " (924, [439, 2409, 6995, 11860, 15416, 43748, 45881]),\n",
              " (9020, [9021, 9016, 9017, 9022, 317, 9023]),\n",
              " (9993, [9991, 13134, 13478, 13877, 34299, 34485, 34642, 37941]),\n",
              " (9992, [9987, 9989, 35667, 9989, 9991, 9991]),\n",
              " (9019, [9022, 317, 9023]),\n",
              " (9022, [9019, 9020, 9021, 317, 9016, 9017, 9023]),\n",
              " (8941, [8943, 8944, 8940])]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recommend_list.saveAsTextFile(\"/content/drive/MyDrive/Output.txt\")"
      ],
      "metadata": {
        "id": "nynIe2oCRgaM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}